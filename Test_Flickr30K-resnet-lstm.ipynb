{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!pip install rouge-score jiwer\n",
    "!pip install nltk \n",
    "!pip install pandas \n",
    "!pip install numpy \n",
    "!pip install spacy \n",
    "!pip install sklearn \n",
    "!pip install matplotlib \n",
    "!pip install pillow\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1740035437443,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "RDrflirOTtAa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Imported Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpYKDYpNk5wV"
   },
   "source": [
    "Importing Images datasets from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65055,
     "status": "ok",
     "timestamp": 1740035504496,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "nauIBeh113B1",
    "outputId": "c9df8369-79e4-44ec-a5ab-70bf889ae452"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d eeshawn/flickr30k\n",
    "\n",
    "# Check if the 'dataset' folder exists. If not, create it.\n",
    "if not os.path.exists('dataset'):\n",
    "    os.makedirs('dataset')\n",
    "\n",
    "# Unzip the file into the 'dataset' folder.\n",
    "# !unzip flickr30k.zip -d dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djk78YYzlIs9"
   },
   "source": [
    "Importing text data from Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1739981161517,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "8BOOjIAC2qI2",
    "outputId": "2cd6ebd7-9b28-4480-ae74-12bfa715f90a"
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip -P dataset/\n",
    "# !unzip dataset/Flickr8k_text.zip -d dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8dDOchNlOnk"
   },
   "source": [
    "Delete zip files and keep unzipped folders and files only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQ6BVmhL2G8c"
   },
   "outputs": [],
   "source": [
    "# shutil.rmtree('dataset/__MACOSX', ignore_errors=True)\n",
    "# if os.path.exists('dataset/Flickr8k_Dataset.zip'):\n",
    "#     os.remove('dataset/Flickr8k_Dataset.zip')\n",
    "# if os.path.exists('dataset/Flickr8k_text.zip'):\n",
    "#     os.remove('dataset/Flickr8k_text.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740035536913,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "UU1o6TMpTtAc"
   },
   "outputs": [],
   "source": [
    "!ls /workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1740035536913,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "UU1o6TMpTtAc"
   },
   "outputs": [],
   "source": [
    "image_data_location = \"/workspace/flickr30k_images\"\n",
    "caption_data_location = \"/workspace/captions.txt\"\n",
    "\n",
    "print(os.path.join(image_data_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVvSyuHHlZRM"
   },
   "source": [
    "It seems some images are not good , Collecting all the images which are opening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2561,
     "status": "ok",
     "timestamp": 1740035717979,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "lScWCg24YK8d",
    "outputId": "7926c67e-94d4-45dc-8a42-d5f44ce84289"
   },
   "outputs": [],
   "source": [
    "#  to collect images from image_data_location folder which are getting opened\n",
    "\n",
    "images_good = []\n",
    "# iterate over all the files in the image_data_location\n",
    "for filename in os.listdir(image_data_location):\n",
    "  # check if the file is an image\n",
    "  if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "    # open the image\n",
    "    try:\n",
    "      img = Image.open(os.path.join(image_data_location, filename))\n",
    "      # images_good.append(filename.split(\".\")[0]) #append to a list\n",
    "      images_good.append(filename) #append to a list\n",
    "    except IOError:\n",
    "      print(\"Unable to open image:\", filename)\n",
    "\n",
    "# Now you have a list 'images' containing opened image objects from the specified directory.\n",
    "# You can further process these images as needed.\n",
    "print(f\"Loaded {len(images_good)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzWr4n3ylnTE"
   },
   "source": [
    "Collecting the dataframe from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1740036229584,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "D773ntrmTtAd",
    "outputId": "56d4f226-6972-4efc-933a-0bf63a378dea"
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(caption_data_location, sep=\",\", header=None, names=['image','comment_number','captions'], skiprows=1)\n",
    "raw_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnqHzCJ07USW"
   },
   "outputs": [],
   "source": [
    "# The below code is to clear out the part post the image name ex: 1000268201_693b08cb0e.jpg#0\t --> 1000268201_693b08cb0e.jpg\n",
    "# raw_df['image'] = raw_df['image'].str.split('.').str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGHE2MYXluSE"
   },
   "source": [
    "Compare good images list with dataframe and identifying the indexes of bad images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51775,
     "status": "ok",
     "timestamp": 1740036080035,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "D4JTPTDkZeFD"
   },
   "outputs": [],
   "source": [
    "#  to find the df[image] rows not in images_good list\n",
    "# apprently 30K data set is good. So no need to do the below code.\n",
    "\n",
    "# indices_not_in_list = []\n",
    "# for index, image_name in raw_df['image'].items():\n",
    "#     if image_name not in images_good:\n",
    "#         # print(image_name)\n",
    "#         indices_not_in_list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1740036215494,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "az-6uJCRnEmJ",
    "outputId": "01f4bb93-b7b2-4ead-babf-8e8e384a419b"
   },
   "outputs": [],
   "source": [
    "# print(indices_not_in_list)\n",
    "# len(indices_not_in_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFd2tv7Hl4xQ"
   },
   "source": [
    "Dropping bad images captions from captions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1740037665341,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "ALcufyAMokA2"
   },
   "outputs": [],
   "source": [
    "# df = raw_df.drop(indices_not_in_list, inplace=False)\n",
    "df = raw_df.drop('comment_number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1740037667809,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "YP3z7Wg6ZRes",
    "outputId": "0466dc87-3eae-4d6f-b3ee-b302ac74bd28"
   },
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(f\"raw df shape - {raw_df.shape}\")\n",
    "print(f\"new df shape - {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XgTq1SAl-Zd"
   },
   "source": [
    "Find out the caption which is having maximum words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1740037688057,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "jTEhRErrsNbM",
    "outputId": "b4ca7b34-bb59-433b-9060-92d897d3f568"
   },
   "outputs": [],
   "source": [
    "# to find the maximum words string from df['captions'] column\n",
    "\n",
    "df['word_count'] = df['captions'].apply(lambda x: len(x.split()))\n",
    "max_words_string = df.loc[df['word_count'].idxmax(), 'captions']\n",
    "print(f\"The string with the maximum words is:\\n{max_words_string}\")\n",
    "print(f\"the length of the string is {len(max_words_string.split())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kalRFW4MmJnD"
   },
   "source": [
    "randomly printing an image and caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1740037706516,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "n8R2y7gDTtAe",
    "outputId": "c69dd21d-66f7-42e8-cc1a-08d146845c10"
   },
   "outputs": [],
   "source": [
    "data_idx = 11\n",
    "image_name = df.iloc[data_idx,0]\n",
    "# print(image_name)\n",
    "image_path = image_data_location + \"/\" + image_name\n",
    "print(image_path)\n",
    "img = mpimg.imread(image_path)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "for i in range(data_idx, data_idx+5):\n",
    "    print(f\"Caption - {df.iloc[i,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPBdMDUqmdqE"
   },
   "source": [
    "Spacy library :\n",
    "spaCy is used for Tokenization: Breaks down text into individual words or tokens.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16219,
     "status": "ok",
     "timestamp": 1740037729765,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "sHHX3Ne5TtAf"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    spacy_eng = spacy.load('en_core_web_sm')\n",
    "    print(\"Model loaded successfully!\")\n",
    "except OSError:\n",
    "    print(\"Failed to load the model. Try reinstalling.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 919,
     "status": "ok",
     "timestamp": 1740037734033,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "ww0APLfFTtAf",
    "outputId": "58738edd-f0ab-43c1-8d14-30fbcffadf5a"
   },
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load('en_core_web_sm')\n",
    "text = \"This is a good place to find a city\"\n",
    "[token.text.lower() for token in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeoeTiNdqDSc"
   },
   "source": [
    "Class to add special tokens , tokenization and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740037736290,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "ouWUfZX1TtAf"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self,freq_threshold):\n",
    "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocab(self,sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenize(sentence):\n",
    "                frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self,text):\n",
    "        tokenized_text = self.tokenize(text)\n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740037739758,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "kMtDpJblTtAg",
    "outputId": "4531d734-4355-482d-c5ec-1d298b275ecd"
   },
   "outputs": [],
   "source": [
    "v = Vocabulary(freq_threshold=1) #was1\n",
    "v.build_vocab([\"This is a new city\"])\n",
    "print(v.stoi)\n",
    "print(v.itos)\n",
    "print(v.numericalize(\"This is a new city\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab2Q_tUKq5Jc"
   },
   "source": [
    "Dataset preparation\n",
    "Images and Captions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1740037805740,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "jQ7AtreMTtAh"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,root_dir,captions_df,transform=None, freq_threshold=5): # was freq_threshold=5\n",
    "        self.root_dir = root_dir\n",
    "        self.df = captions_df\n",
    "\n",
    "\n",
    "        self.transform = transform\n",
    "        self.imgs = self.df[\"image\"] .astype(str)\n",
    "        self.captions = self.df[\"captions\"]\n",
    "\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        caption = self.captions[idx]\n",
    "        img_name = self.imgs[idx]\n",
    "\n",
    "        img_location = os.path.join(self.root_dir,img_name)\n",
    "        img = Image.open(img_location).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        caption_vec = []\n",
    "        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n",
    "        caption_vec += self.vocab.numericalize(caption)\n",
    "        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n",
    "        #print(f\"caption - {caption}\")\n",
    "\n",
    "        return img, torch.tensor(caption_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740037809445,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "S9ryBdHcTtAi"
   },
   "outputs": [],
   "source": [
    "#defing the transform to be applied\n",
    "transforms = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1740037810412,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "tOJ3Fs7-TtAi"
   },
   "outputs": [],
   "source": [
    "def show_image(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor\"\"\"\n",
    "    inp = inp.numpy().transpose((1,2,0))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6147,
     "status": "ok",
     "timestamp": 1740037818405,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "E-Re0M0vTtAi"
   },
   "outputs": [],
   "source": [
    "# testing the dataset\n",
    "dataset = CustomDataset(\n",
    "        root_dir = image_data_location,\n",
    "        captions_df = df,\n",
    "        transform = transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740037819536,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "1i8Wrf437o1K"
   },
   "outputs": [],
   "source": [
    "def token_to_sentence(dataset, caps):\n",
    "  special_strings = [\"<SOS>\", \"<UNK>\", \"<EOS>\",\"<PAD>\"]\n",
    "  sen = [dataset.vocab.itos[token] for token in caps.tolist()]\n",
    "  sen = [token for token in sen if token not in special_strings]\n",
    "  return \" \".join(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1740037822042,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "YJ0fWw1JTtAi",
    "outputId": "3d738b10-9db7-4dc3-8e5a-b35b4264b641"
   },
   "outputs": [],
   "source": [
    "img, caps = dataset[0]\n",
    "show_image(img,\"Image\")\n",
    "print(\"Token :\", caps)\n",
    "print(\"Sentence: \")\n",
    "sen = token_to_sentence(dataset, caps)\n",
    "print(sen)\n",
    "# sen = [dataset.vocab.itos[token] for token in caps.tolist()]\n",
    "# special_strings = [\"<SOS>\", \"<UNK>\", \"<EOS>\"]\n",
    "# sen = [token for token in sen if token not in special_strings]\n",
    "# print(\" \".join(sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsZBbcYVuYxc"
   },
   "source": [
    "Collate function in a PyTorch\n",
    "\n",
    "1) Collate Images: Combine a batch of images into a single tensor.\n",
    "\n",
    "2) Pad Captions: Pad captions to a uniform length using a special padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1740037829287,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "4Vakly58TtAi"
   },
   "outputs": [],
   "source": [
    "class CapsCollate:\n",
    "    def __init__(self,pad_idx,batch_first=False):\n",
    "        self.pad_idx = pad_idx\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def __call__(self,batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "#         print(f\"shape - {(imgs)}\")\n",
    "#         print(\"----\"*22)\n",
    "        imgs = torch.cat(imgs,dim=0)\n",
    "#         print(f\"shape - {imgs}\")\n",
    "#         print(\"------\")\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n",
    "        return imgs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)  # Should print the installed version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wic0sPawH2_8"
   },
   "source": [
    " create train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1445,
     "status": "ok",
     "timestamp": 1740037833840,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "sVIu2zA6PcP8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5694,
     "status": "ok",
     "timestamp": 1740037840313,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "UUcYCZkGH0Ys"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Reset indices\n",
    "train_df = train_df.reset_index(drop=True)  # Reset index for train_df\n",
    "val_df = val_df.reset_index(drop=True)    # Reset index for val_df\n",
    "test_df = test_df.reset_index(drop=True)   # Reset index for test_df\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(root_dir=image_data_location, captions_df=train_df, transform=transforms)\n",
    "val_dataset = CustomDataset(root_dir=image_data_location, captions_df=val_df, transform=transforms)\n",
    "test_dataset = CustomDataset(root_dir=image_data_location, captions_df=test_df, transform=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Vc2F7ECupSC"
   },
   "source": [
    "Creating Dataloaders with batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740037845378,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "rUp8e0lrTtAj"
   },
   "outputs": [],
   "source": [
    "#writing the dataloader\n",
    "#setting the constants\n",
    "BATCH_SIZE = 35 #Was 4\n",
    "NUM_WORKER = 2 #was 1\n",
    "\n",
    "#token to represent the padding\n",
    "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=True,\n",
    "    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DosbAPaIPI9"
   },
   "source": [
    "Creating Train Val and Test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740037848115,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "gQtVaMuCIMKd"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=True,\n",
    "    collate_fn=CapsCollate(pad_idx=pad_idx, batch_first=True)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=False,\n",
    "    collate_fn=CapsCollate(pad_idx=pad_idx, batch_first=True)\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKER,\n",
    "    shuffle=False,\n",
    "    collate_fn=CapsCollate(pad_idx=pad_idx, batch_first=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the iterator from the dataloader\n",
    "dataiter = iter(data_loader)\n",
    "\n",
    "#getting the next batch\n",
    "batch = next(dataiter)\n",
    "\n",
    "#unpacking the batch\n",
    "images, captions = batch\n",
    "\n",
    "#showing info of image in single batch\n",
    "for i in range(BATCH_SIZE):\n",
    "    img,cap = images[i],captions[i]\n",
    "    #print(f\"captions - {captions[i]}\")\n",
    "    caption_label = [dataset.vocab.itos[token] for token in cap.tolist()]\n",
    "    eos_index = caption_label.index('<EOS>')\n",
    "    caption_label = caption_label[1:eos_index]\n",
    "    caption_label = ' '.join(caption_label)\n",
    "    show_image(img,caption_label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7V299N1vB5N"
   },
   "source": [
    "Below code is to understand way of working of the  dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740037876857,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "PrtrLD9NTtAj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWYGAe3YvgR7"
   },
   "source": [
    "Model Architecture of Encoder and Decoder with caption generation method\n",
    "\n",
    "Need to modify this to CLIP model with self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1740037896777,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "Gorqj4-6T1gL"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self,embed_size):\n",
    "        super(EncoderCNN,self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features,embed_size)\n",
    "\n",
    "    def forward(self,images):\n",
    "        features = self.resnet(images)\n",
    "#\n",
    "        features = features.view(features.size(0),-1)\n",
    "#\n",
    "        features = self.embed(features)\n",
    "#\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers=1,drop_prob=0.3):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers=num_layers,batch_first=True)\n",
    "        self.fcn = nn.Linear(hidden_size,vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self,features, captions):\n",
    "        # vectorize the caption\n",
    "#\n",
    "        embeds = self.embedding(captions[:,:-1])\n",
    "#\n",
    "        x = torch.cat((features.unsqueeze(1),embeds),dim=1)\n",
    "#\n",
    "        x,_ = self.lstm(x)\n",
    "#\n",
    "        x = self.fcn(x)\n",
    "#\n",
    "        return x\n",
    "\n",
    "    def generate_caption(self,inputs,hidden=None,max_len=20,vocab=None): #hidden=None\n",
    "        # Inference part\n",
    "        # Given the image features generate the captions\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        captions = []\n",
    "\n",
    "        if hidden is None:\n",
    "          hidden = (torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(inputs.device),\n",
    "                    torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(inputs.device))\n",
    "\n",
    "         # Ensure inputs is 3D: [batch_size, 1, embed_size]\n",
    "        inputs = inputs.unsqueeze(1)  # Add sequence length = 1\n",
    "\n",
    "        for i in range(max_len):\n",
    "\n",
    "            output, hidden = self.lstm(inputs, hidden)  # LSTM expects 3D input\n",
    "            output = self.fcn(output.squeeze(1))  # Shape: [batch_size, vocab_size]\n",
    "            #output = output.view(batch_size,-1)\n",
    "\n",
    "\n",
    "\n",
    "            #select the word with most val\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "\n",
    "            #save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "\n",
    "            #end if <EOS detected>\n",
    "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "\n",
    "            #send generated word as the next caption\n",
    "            inputs = self.embedding(predicted_word_idx.unsqueeze(1)) # Shape: [batch_size, 1, embed_size]\n",
    "\n",
    "\n",
    "        #covert the vocab idx to words and return sentence\n",
    "        return [vocab.itos[idx] for idx in captions]\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers=1,drop_prob=0.3):\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size,hidden_size,vocab_size,num_layers,drop_prob)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1740037903809,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "RgvfgKBKTtAl",
    "outputId": "567f1c99-b493-4806-fb4b-4135febd876f"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740037907412,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "-mQBCFspTtAm"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 512 #400\n",
    "hidden_size = 512\n",
    "vocab_size = len(dataset.vocab)\n",
    "num_layers = 2\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1983,
     "status": "ok",
     "timestamp": 1740037913747,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "40fYsnkyTtAm",
    "outputId": "90f47f2a-35ed-48c5-fa78-e525a04863fa"
   },
   "outputs": [],
   "source": [
    "# initialize model, loss etc\n",
    "model = EncoderDecoder(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11393,
     "status": "ok",
     "timestamp": 1740037928730,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "-kDWXOJLFQYO",
    "outputId": "2641f63c-1878-4421-9f48-34161a4bffd1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download data for tokenization\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "# Ensure NLTK's tokenizer is downloaded (Run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Edited - SK\n",
    "!pip install rouge-score jiwer\n",
    "from rouge_score import rouge_scorer\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1740037931013,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "XD0YNPg1wcLe"
   },
   "outputs": [],
   "source": [
    "special_tokens = [\"<SOS>\", \"<UNK>\", \"<EOS>\",\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1740037932540,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "X-vnFMhkyi9T"
   },
   "outputs": [],
   "source": [
    "smoothie = SmoothingFunction().method4  # Choose a smoothing method\n",
    "print(\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4606091,
     "status": "ok",
     "timestamp": 1739988006224,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "MihU6nOeTtAm",
    "outputId": "0c7882eb-5f63-454c-9952-948d4a702fe7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Execution started\")\n",
    "from datetime import datetime\n",
    "# Print current timestamp\n",
    "print(\"Start Timestamp:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_blue_scores = []\n",
    "train_vlue_scores = []\n",
    "train_vlue_rouge_scores = []\n",
    "train_vlue_wer_scores = []\n",
    "val_rouge_scores = []\n",
    "val_wer_scores = []\n",
    "print_every = 6000\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    print(\"Epoch: {} StartTime: {}\".format(epoch, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    train_loss = 0\n",
    "    for idx, (image, captions) in enumerate(iter(train_loader)):\n",
    "        image, captions = image.to(device), captions.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        for img in image:\n",
    "          reference_captions = token_to_sentence(train_dataset,captions[0])\n",
    "          generated_captions = model.decoder.generate_caption(model.encoder(img.unsqueeze(0)),vocab=dataset.vocab)\n",
    "          generated_captions = [token for token in generated_captions if token not in special_tokens]\n",
    "          generated_captions_str = ' '.join(generated_captions)\n",
    "          #Compute BLUE-4 Scores\n",
    "          bleu4_score = sentence_bleu(reference_captions, generated_captions, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie )\n",
    "          # Edited - SK\n",
    "          scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "          rouge_scores = scorer.score(reference_captions, generated_captions_str)\n",
    "          wer_score = wer(reference_captions, generated_captions_str)\n",
    "\n",
    "          #bleu4_score = sentence_bleu(reference_captions, generated_captions, weights=(1, 0, 0, 0))\n",
    "          train_vlue_scores.append(bleu4_score)\n",
    "          train_vlue_rouge_scores.append(rouge_scores)\n",
    "          train_vlue_wer_scores.append(wer_score)\n",
    "\n",
    "    #calculate the train loss\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    print(\"Epoch: {} loss: {:.5f}\".format(epoch, train_loss))\n",
    "\n",
    "    # if (idx + 1) % print_every == 0:\n",
    "    #   print(\"Epoch: {} loss: {:.5f}\".format(epoch, loss.item()))\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for val_image, val_captions in val_loader:\n",
    "            val_image, val_captions = val_image.to(device), val_captions.to(device)\n",
    "            outputs = model(val_image, val_captions)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), val_captions.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Generate captions for the current batch of images\n",
    "            for image in val_image:\n",
    "              ref_caption = token_to_sentence(val_dataset,val_captions[0])\n",
    "              print(\"Reference Caption:\", ref_caption)\n",
    "              generated_captions = model.decoder.generate_caption(model.encoder(image.unsqueeze(0)),vocab=dataset.vocab)\n",
    "              generated_captions = [token for token in generated_captions if token not in special_tokens]\n",
    "              generated_captions_str = ' '.join(generated_captions) # convert to string\n",
    "              print(\"Generated Caption:\", ' '.join(generated_captions))  # Print the generated caption\n",
    "              # Compute BLEU-4 Score\n",
    "              bleu4_score = sentence_bleu(ref_caption, generated_captions, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie)\n",
    "\n",
    "              scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "              rouge_scores = scorer.score(ref_caption, generated_captions_str)\n",
    "              wer_score = wer(ref_caption, generated_captions_str)\n",
    "              #bleu4_score = sentence_bleu(ref_caption, generated_captions, weights=(1, 0, 0, 0))\n",
    "              val_blue_scores.append(bleu4_score)\n",
    "              val_rouge_scores.append(rouge_scores)\n",
    "              val_wer_scores.append(wer_score)\n",
    "              print(\"BLEU-4 Score: {:.5f}\".format(bleu4_score))\n",
    "              # print(\"ROUGE Score: {:.5f}\".format(rouge_scores))\n",
    "              for key, value in rouge_scores.items():\n",
    "                print(f\"ROUGE {key}: {value.fmeasure:.5f}\")\n",
    "              print(\"WER Score: {:.5f}\".format(wer_score))\n",
    "\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        print(\"Epoch: {} Validation loss: {:.5f}\".format(epoch, val_loss))\n",
    "        print(\"Epoch: {} EndTime: {}\".format(epoch, datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1739988131997,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "1D3FD-BWiPag",
    "outputId": "5fbde0ea-c937-4f52-8aa6-729bfd78897f"
   },
   "outputs": [],
   "source": [
    "print(f\"Average train loss: {np.mean(train_losses)}\")\n",
    "print(f\"Average validation loss: {np.mean(val_losses)}\")\n",
    "print(f\"Average train BLEU-4 Score: {np.mean(train_vlue_scores)}\")\n",
    "print(f\"Average val BLEU-4 Score: {np.mean(val_blue_scores)}\")\n",
    "\n",
    "# print(f\"Average train Rouge Score: {np.mean(train_vlue_rouge_scores)}\")\n",
    "# print(f\"Average val Rouge Score: {np.mean(val_rouge_scores)}\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Extract specific Rouge scores (e.g., 'rouge1' F1 scores)\n",
    "rouge1_scores = [d['rouge1'].fmeasure for d in train_vlue_rouge_scores]\n",
    "\n",
    "# Calculate the mean of the extracted scores\n",
    "average_rouge1_score = np.mean(rouge1_scores)\n",
    "\n",
    "# Print the average score\n",
    "print(f\"Average train Rouge-1 F1 Score: {average_rouge1_sco\n",
    "print(f\"Average train WER Score: {np.mean(train_vlue_wer_scores)}\")\n",
    "print(f\"Average val WER Score: {np.mean(val_wer_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbpaqbrjxrR7"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# torch.save(model.state_dict(), 'image_captioning_model.pth')\n",
    "torch.save(model.state_dict(), 'image_captioning_model.pth')\n",
    "torch.save(model.state_dict(), 'Flickr8K-resnet-lstm.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1739988246505,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "_v-vpmv9wxJQ",
    "outputId": "26d2acad-efe3-4798-ec65-178d0696d2ae"
   },
   "outputs": [],
   "source": [
    "# prompt: provide code to download image_captioning_model.pth file from colab to local\n",
    "\n",
    "from google.colab import files\n",
    "files.download('image_captioning_model.pth')\n",
    "files.download('Flickr8K-resnet-lstm.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1739988401350,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "XEFpg9Gafo2p",
    "outputId": "86b41181-c7a8-4fee-94f7-922a4b084792"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the training losses\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1739988412057,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "OcOIVPI2fyKf",
    "outputId": "04ee5b0c-512d-4b5a-f67a-1d91918de35c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot the validation losses\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 118,
     "status": "ok",
     "timestamp": 1739988441584,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "-EcqkUSlOMg5",
    "outputId": "389052b7-c302-4064-8569-440acc1c5c3a"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Ensure NLTK's tokenizer is downloaded (Run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87936,
     "status": "ok",
     "timestamp": 1739988533748,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "2jGiD7r9Jpeb",
    "outputId": "e854f43c-f5b8-4706-90d0-09aa7b4d4f85"
   },
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "blu_scores = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient calculation\n",
    "with torch.no_grad():\n",
    "    for test_image, test_captions in test_loader:\n",
    "        test_image, test_captions = test_image.to(device), test_captions.to(device)\n",
    "\n",
    "        for i, image in enumerate(test_image):\n",
    "            # Generate captions using the model\n",
    "            generated_caption = model.decoder.generate_caption(\n",
    "                model.encoder(image.unsqueeze(0)), vocab=dataset.vocab\n",
    "            )\n",
    "\n",
    "            # Convert generated caption to tokens\n",
    "            generated_caption = [token for token in generated_caption if token not in special_tokens]\n",
    "            generated_caption = \" \".join(generated_caption)\n",
    "            reference_captions = token_to_sentence(test_dataset,test_captions[0])\n",
    "\n",
    "\n",
    "            # Compute BLEU-4 Score\n",
    "            bleu4_score = sentence_bleu(reference_captions, generated_caption, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "            blu_scores.append(bleu4_score)\n",
    "\n",
    "            # Print results\n",
    "            print(f\"Reference Captions: {reference_captions}\")\n",
    "            print(f\"Generated Caption: {generated_caption}\")\n",
    "            print(f\"BLEU-4 Score: {bleu4_score:.4f}\\n\")\n",
    "\n",
    "print(f\"Average BLEU-4 Score: {np.mean(blu_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1739988605699,
     "user": {
      "displayName": "Srikrishna Dasu",
      "userId": "05074597022229387499"
     },
     "user_tz": -330
    },
    "id": "mGoRnxz9irAw",
    "outputId": "c19e6ca6-2ce7-40dd-c183-2fa29d96a95a"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming 'blu_scores' is a list of BLEU scores from the previous code\n",
    "# and you have a corresponding list of image indices or identifiers.\n",
    "\n",
    "# Example image identifiers (replace with your actual identifiers)\n",
    "image_ids = list(range(len(blu_scores)))\n",
    "\n",
    "plt.figure(figsize=(10, 10))  # Adjust figure size as needed\n",
    "plt.scatter(image_ids, blu_scores, color='blue', label='BLEU Scores')\n",
    "plt.xlabel('Image ID')\n",
    "plt.ylabel('BLEU Score')\n",
    "plt.title('BLEU Scores for Images')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1LdxBr8zOpjqE-k3vnNbaJaQUMWhpezwG",
     "timestamp": 1739988618361
    },
    {
     "file_id": "1TxnXwR58uw_2cZ5E24JoHpT49aS6ucFV",
     "timestamp": 1739967509920
    },
    {
     "file_id": "1nFHo-i71xSO3hDeR3fajD5j3IlexWq1i",
     "timestamp": 1737252402360
    },
    {
     "file_id": "1AVJ9DTtVSVX02TDTO1yOoClaLUs3xkpi",
     "timestamp": 1737192843743
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 623289,
     "sourceId": 1111676,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30163,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
